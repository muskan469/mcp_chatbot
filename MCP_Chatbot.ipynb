{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNz95+BvctYMwWTtny6gCO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muskan469/mcp_chatbot/blob/main/MCP_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIhPhJsWTt3D",
        "outputId": "ab8e86e7-2009-4d44-d412-594db0cf58c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=0664bdc7765a8a7faed4aa6f5fa997f01055189a50e038ea21e7dd279a72ea6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3jbc5xoTzF3",
        "outputId": "58162dbf-4a7c-41cd-dfeb-1d1587980d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KrUZqSYMhoz",
        "outputId": "8a17bc33-8088-452e-8082-30876cf33c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCSuldRxT4ME",
        "outputId": "7bc4c50b-9aaf-427c-a6a1-96a2707d5121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.52.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Downloading anthropic-0.52.0-py3-none-any.whl (286 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/286.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.52.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLQDrbc2TOKv"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import json\n",
        "import os\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "v_Ax-HZJOjz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAPER_DIR = \"papers\""
      ],
      "metadata": {
        "id": "p63J3WzGTtTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_papers(topic: str, max_results: int= 5) -> List[str]:\n",
        "  \"\"\"\n",
        "  Search for papers on arXiv based on a topic and store their information.\n",
        "\n",
        "  Args:\n",
        "    topic: The topic to search for\n",
        "    max_results : Maximum number of results to retrieve(default:5)\n",
        "\n",
        "    Returns:\n",
        "      List of paper IDs found in the search\n",
        "    \"\"\"\n",
        "\n",
        "  #Use arxiv to find the papers\n",
        "  client = arxiv.Client()\n",
        "\n",
        "  #Search for the most relevant articles matching the queried topic\n",
        "  search = arxiv.Search(\n",
        "      query = topic,\n",
        "      max_results = max_results,\n",
        "      sort_by = arxiv.SortCriterion.Relevance\n",
        "  )\n",
        "\n",
        "  papers = client.results(search)\n",
        "\n",
        "  #Create directory for this topic\n",
        "  path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "\n",
        "  file_path = os.path.join(path, \"papers_info.json\")\n",
        "\n",
        "  #Try to load existing papers info\n",
        "  try:\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "      papers_info = json.load(json_file)\n",
        "  except (FileNotFoundError, json.JSONDecodeError):\n",
        "    papers_info = {}\n",
        "\n",
        "  #Process each paper and add to papers_info\n",
        "  paper_ids = []\n",
        "  for paper in papers:\n",
        "    paper_ids.append(paper.get_short_id())\n",
        "    paper_info = {\n",
        "        'title': paper.title,\n",
        "        'authors': [author.name for author in paper.authors],\n",
        "        'summary': paper.summary,\n",
        "        'pdf_url': paper.pdf_url,\n",
        "        'published': str(paper.published.date())\n",
        "    }\n",
        "    papers_info[paper.get_short_id()] = paper_info\n",
        "\n",
        "  #Save updated papers_info to json file\n",
        "  with open(file_path, \"w\") as json_file:\n",
        "    json.dump(papers_info, json_file, indent=2)\n",
        "\n",
        "  print(f\"Results are saved in: {file_path}\")\n",
        "\n",
        "  return paper_ids"
      ],
      "metadata": {
        "id": "YZQMNmxGTsN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_papers(\"alphaevolve\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdg1g8bPTat9",
        "outputId": "87a9c9e6-8678-4a74-dbe6-584bfac917fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are saved in: papers/alphaevolve/papers_info.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2505.16105v1', '2103.16196v2']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info(paper_id: str) -> str:\n",
        "  \"\"\"\n",
        "  Search for information about a specific paper across all topic directories.\n",
        "\n",
        "  Args:\n",
        "    paper_id: The ID of the paper to look for\n",
        "\n",
        "  Returns:\n",
        "    JSON string with paper information if found, error message if not found\n",
        "  \"\"\"\n",
        "\n",
        "  for item in os.listdir(PAPER_DIR):\n",
        "    item_path = os.path.join(PAPER_DIR, item)\n",
        "    if os.path.isdir(item_path):\n",
        "      file_path = os.path.join(item_path, \"papers_info.json\")\n",
        "      if os.path.isfile(file_path):\n",
        "        try:\n",
        "          with open(file_path, \"r\") as json_file:\n",
        "            papers_info = json.load(json_file)\n",
        "            if paper_id in papers_info:\n",
        "              return json.dumps(papers_info[paper_id], indent=2)\n",
        "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "          print(f\"Error reading {file_path}: {str(e)}\")\n",
        "          continue\n",
        "\n",
        "    return f\"There is no saved information related to paper {paper_id}.\"\n"
      ],
      "metadata": {
        "id": "EZxHhF8gWLiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_info(\"2505.16105v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "dAZBBeOIXOJT",
        "outputId": "87f261c0-3b86-47b9-ec97-b932dae397ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"title\": \"Sums and differences of sets (improvement over AlphaEvolve)\",\\n  \"authors\": [\\n    \"Robert Gerbicz\"\\n  ],\\n  \"summary\": \"On May 14, 2025, DeepMind announced that AlphaEvolve, a large language model\\\\napplied to a set of mathematical problems, had matched or exceeded the best\\\\nknown bounds on several problems. In the case of the sum and difference of sets\\\\nproblem, AlphaEvolve, using a set of $54265$ integers, improved the known lower\\\\nbound of $\\\\\\\\theta=1.14465$ to $\\\\\\\\theta=1.1584$. In this paper, we present an\\\\nimproved bound $\\\\\\\\theta=1.173050$ using an explicit construction of a U set that\\\\ncontains more than $10^{43546}$ elements. For fast integer and floating-point\\\\narithmetic, we used the (free) GMP library.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/2505.16105v1\",\\n  \"published\": \"2025-05-22\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool schema"
      ],
      "metadata": {
        "id": "q5b4pbuoXRMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.generativeai.protos import FunctionDeclaration, Tool, Schema, Type\n",
        "tools = [\n",
        "    Tool(\n",
        "        function_declarations=[\n",
        "            FunctionDeclaration(\n",
        "                name=\"search_papers\",\n",
        "                description=\"Search for papers on arXiv based on a topic and store their information.\",\n",
        "                parameters=Schema(\n",
        "                    type=Type.OBJECT,\n",
        "                    properties={\n",
        "                        \"topic\": Schema(\n",
        "                            type=Type.STRING,\n",
        "                            description=\"The topic to search for\"\n",
        "                        ),\n",
        "                        \"max_results\": Schema(\n",
        "                            type=Type.INTEGER,\n",
        "                            description=\"Maximum number of results to retrieve\",\n",
        "                            # default=5\n",
        "                        )\n",
        "                    },\n",
        "                    required=[\"topic\"]\n",
        "                ),\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    Tool(\n",
        "        function_declarations=[\n",
        "            FunctionDeclaration(\n",
        "                name=\"extract_info\",\n",
        "                description=\"Search for information about a specific paper across all topic directories.\",\n",
        "                parameters=Schema(\n",
        "                    type=Type.OBJECT,\n",
        "                    properties={\n",
        "                        \"paper_id\": Schema(\n",
        "                            type=Type.STRING,\n",
        "                            description=\"The ID of the paper to look for\"\n",
        "                        )\n",
        "                    },\n",
        "                    required=[\"paper_id\"]\n",
        "                ),\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "SGF529h-Uga0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool Mapping"
      ],
      "metadata": {
        "id": "fc2daOH8Xpze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_tool_function = {\n",
        "    \"search_papers\": search_papers,\n",
        "    \"extract_info\": extract_info\n",
        "}\n",
        "\n",
        "def execute_tool(tool_name, tool_args):\n",
        "\n",
        "  result = mapping_tool_function[tool_name](**tool_args)\n",
        "\n",
        "  if result is None:\n",
        "      result = \"The operation completed but didn't return any results.\"\n",
        "\n",
        "  elif isinstance(result, list):\n",
        "      result = ', '.join(result)\n",
        "\n",
        "  elif isinstance(result, dict):\n",
        "      # Convert dictionaries to formatted JSON strings\n",
        "      result = json.dumps(result, indent=2)\n",
        "\n",
        "  else:\n",
        "      # For any other type, convert using str()\n",
        "      result = str(result)\n",
        "  return result"
      ],
      "metadata": {
        "id": "n_g9qFzCXrXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot Code"
      ],
      "metadata": {
        "id": "Y0yMvUVBX_uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load_dotenv()\n",
        "# client = anthropic.Anthropic()"
      ],
      "metadata": {
        "id": "GgIpJ2V-X9Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5PDMkoJYxLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Gemini model\n",
        "\n",
        "def trial_function():\n",
        "# model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20', tools=tools)\n",
        "  model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20', tools=tools)\n",
        "# , tools=tools)\n",
        "\n",
        "#Start a chat session\n",
        "  chat_session = model.start_chat(history=[])\n",
        "\n",
        "#Send the user query\n",
        "  response = chat_session.send_message(\"What is United States Of America?\")\n",
        "\n",
        "  print(response)\n",
        "\n",
        "trial_function()\n"
      ],
      "metadata": {
        "id": "oUzBaNFQQzgf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "1cbc8c8c-836c-4189-d169-5a67f267ce25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"I cannot answer general knowledge questions. My purpose is to assist with tasks related to searching and extracting information about research papers.\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"index\": 0\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 137,\n",
            "        \"candidates_token_count\": 24,\n",
            "        \"total_token_count\": 161\n",
            "      },\n",
            "      \"model_version\": \"models/gemini-2.5-flash-preview-05-20\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query processing"
      ],
      "metadata": {
        "id": "Z1k13LYBYHIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query):\n",
        "\n",
        "  #Define the Gemini model\n",
        "  model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20', tools=tools)\n",
        "\n",
        "  #Start a chat session\n",
        "  chat_session = model.start_chat(history=[])\n",
        "\n",
        "  #Send the user query\n",
        "  response = chat_session.send_message(query)\n",
        "\n",
        "  # print(f\"Intial responses from chatbot: {response}\")\n",
        "\n",
        "  process_query=True\n",
        "  while process_query:\n",
        "    assistant_content = []\n",
        "    tool_calls_made = False\n",
        "\n",
        "    #Process the response\n",
        "    for part in response.candidates[0].content.parts:\n",
        "      if part.text:\n",
        "        print(part.text)\n",
        "        assistant_content.append({\"text\": part.text})\n",
        "        if len(response.candidates[0].content.parts) == 1 and not part.function_call:\n",
        "          process_query = False\n",
        "\n",
        "      if part.function_call:\n",
        "        tool_calls_made = True\n",
        "        tool_name = part.function_call.name\n",
        "        tool_args = part.function_call.args\n",
        "\n",
        "        print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
        "\n",
        "        #Execute tool\n",
        "        result = execute_tool(tool_name, tool_args)\n",
        "\n",
        "        #Send the tool result back to the model\n",
        "        response = chat_session.send_message(genai.protos.Content(\n",
        "            parts=[genai.protos.Part(\n",
        "                function_response=genai.protos.FunctionResponse(\n",
        "                    name=tool_name,\n",
        "                    response={'content': result}\n",
        "                )\n",
        "            )]\n",
        "        ))\n",
        "\n",
        "      if not tool_calls_made and len(response.candidates[0].content.parts) == 1 and response.candidates[0].content.parts[0].text:\n",
        "            process_query = False\n",
        "\n",
        "      if tool_calls_made and not response.candidates[0].content.parts[0].text and not response.candidates[0].content.parts[0].function_call:\n",
        "             process_query = False"
      ],
      "metadata": {
        "id": "IOreyplaYBm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat loop"
      ],
      "metadata": {
        "id": "QPQw2571amtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_loop():\n",
        "  print(\"Type your queries or 'quit' to exit.\")\n",
        "  while True:\n",
        "    try:\n",
        "      query = input(\"\\nQuery: \").strip()\n",
        "      if query.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "      process_query(query)\n",
        "      print(\"\\n\")\n",
        "    except Exception as e:\n",
        "      print(f\"\\nError: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "Z-QnoxrVYVpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "3d9_4-6ObZKp",
        "outputId": "410e7f97-5e07-4833-f17f-0345384eec6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type your queries or 'quit' to exit.\n",
            "\n",
            "Query: give me paper on nuclear physics\n",
            "Calling tool search_papers with args <proto.marshal.collections.maps.MapComposite object at 0x78a7f9100310>\n",
            "Results are saved in: papers/nuclear_physics/papers_info.json\n",
            "I found some papers on nuclear physics. Would you like to know more about any of them?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Query: Yes, tell me the ids of the paper\n",
            "Please specify the topic you want to search for.\n",
            "\n",
            "\n",
            "\n",
            "Query: Tell me about the first paper on nuclear physics\n",
            "I cannot tell you which was the very first paper on nuclear physics without more specific information. Can you tell me what aspects of nuclear physics you are interested in, or perhaps the author or time period?\n",
            "\n",
            "\n",
            "\n",
            "Query: papers for nuclear physics\n",
            "Calling tool search_papers with args <proto.marshal.collections.maps.MapComposite object at 0x78a7f903a0d0>\n",
            "Results are saved in: papers/nuclear_physics/papers_info.json\n",
            "Here are some papers for nuclear physics: 1701.03564v1, 1701.02756v1, 0902.4560v1, 1802.07478v1, nucl-th/0203002v1.\n",
            "\n",
            "\n",
            "\n",
            "Query: tell me more about 1701.03564v1\n",
            "Calling tool extract_info with args <proto.marshal.collections.maps.MapComposite object at 0x78a7f9003150>\n",
            "The paper \"1701.03564v1\" is titled \"Nuclear Symmetry Energy Extracted from Laboratory Experiments\". It was published on January 13, 2017, and authored by Bao-An Li. The paper discusses recent progress and open questions in understanding the density dependence of nuclear symmetry energy from laboratory experiments. You can find the PDF at http://arxiv.org/pdf/1701.03564v1.\n",
            "\n",
            "\n",
            "\n",
            "Query: tell me about donald trump\n",
            "I cannot tell you about Donald Trump. My purpose is to provide information about academic papers. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Query: tell me about the experiment donald trump from arxiv\n",
            "Calling tool search_papers with args <proto.marshal.collections.maps.MapComposite object at 0x78a7f90bd3d0>\n",
            "Results are saved in: papers/the_experiment_donald_trump/papers_info.json\n",
            "Calling tool extract_info with args <proto.marshal.collections.maps.MapComposite object at 0x78a7f91b8ad0>\n",
            "I couldn't find specific information about the paper `the experiment donald trump` using the ID `2011.11688v1`. Let me check if there's any information on the other IDs I found.\n",
            "Calling tool extract_info with args <proto.marshal.collections.maps.MapComposite object at 0x78a8146d7350>\n",
            "Calling tool extract_info with args <proto.marshal.collections.maps.MapComposite object at 0x78a814633bd0>\n",
            "Calling tool extract_info with args <proto.marshal.collections.maps.MapComposite object at 0x78a7fa4f13d0>\n",
            "I am sorry, but I was unable to find any detailed information about the paper \"the experiment donald trump\" from the provided arXiv IDs. It seems there is no saved information related to these papers in the current database.\n",
            "\n",
            "\n",
            "\n",
            "Query: quit\n"
          ]
        }
      ]
    }
  ]
}